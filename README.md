# R-code

Bayesian Semi-Supervised Hidden Markov Models (HMM) for POS-tagging

Bayesian semi-supervised Hidden Markov Models can be used for Part-of-speech-tagging. 
In summary, the HMM is a probabilistic model used to model hidden non-observable states by observing past sequences. 

In this study, which is a part of a group project, we have observed sequences of words, tokens, where their respective POS category such as noun, verb, adjective, etc. are seen as hidden latent states. Given a new sentence with an unobserved state sequence, the models predict the most probable hidden state POS sequence using the Viterbi algorithm.

This type of stochastic method for POS-tagging is frequently used as a preprocessing task in other natural language processing (NLP) analyses.


Recurrent neural network (RNN) from scratch 
Implementation of a one-layer RNN with tanh activation function, output function and softmax function. 

Implementation of a multi-head attention transformer layer from scratch

Used in transformer-based models (GPT, BERT etc.)

Decision tree for prediction from scratch

Algorithm that grows a decision tree used for prediction. 
Note: Good for educational/explorative purpose, but decision trees are not really good as prediction models.

